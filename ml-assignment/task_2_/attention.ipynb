{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1b3dbac",
   "metadata": {},
   "source": [
    "# FUNTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1887eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "460cd2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x, axis=-1):\n",
    "    # Compute softmax values for each set of scores in x along specified axis and returns numpy array of same shape as x with softmax applied\n",
    "    # Subtract max for numerical stability (prevents overflow)\n",
    "    x_shifted = x - np.max(x, axis=axis, keepdims=True)\n",
    "    \n",
    "    # Compute exponentials\n",
    "    exp_x = np.exp(x_shifted)\n",
    "    \n",
    "    # Normalize by sum to get probabilities\n",
    "    return exp_x / np.sum(exp_x, axis=axis, keepdims=True)\n",
    "\n",
    "\n",
    "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
    "        \n",
    "        # Compute Scaled Dot-Product Attention.\n",
    "\n",
    "        # Formula: Attention(Q, K, V) = softmax(Q · K^T / sqrt(d_k)) · V\n",
    "\n",
    "        # Args:\n",
    "        #     Q: Queries (batch_size, seq_len_q, d_k) – what to look for\n",
    "        #     K: Keys    (batch_size, seq_len_k, d_k) – what is available\n",
    "        #     V: Values  (batch_size, seq_len_k, d_v) – info to retrieve\n",
    "        #     mask: Optional boolean mask indicating allowed positions\n",
    "\n",
    "        # Returns:\n",
    "        #     output: Weighted sum of values (batch_size, seq_len_q, d_v)\n",
    "        #     attention_weights: Probabilities for each query-key pair (batch_size, seq_len_q, seq_len_k)\n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "    # Step 1: Compute raw attention scores (Q · K^T)\n",
    "    # This measures similarity between each query and each key\n",
    "    # Result shape: (batch_size, seq_len_q, seq_len_k)\n",
    "    attention_scores = np.matmul(Q, K.transpose(0, 2, 1))\n",
    "    \n",
    "    # Step 2: Scale by square root of key dimension (d_k)\n",
    "    # Scaling prevents dot products from growing too large\n",
    "    # Large values can push softmax into regions with tiny gradients\n",
    "    d_k = K.shape[-1]\n",
    "    scaled_scores = attention_scores / np.sqrt(d_k)\n",
    "    \n",
    "    # Step 3: Apply mask (optional)\n",
    "    # Set masked positions to very large negative value\n",
    "    # After softmax, these become ~0 probability\n",
    "    if mask is not None:\n",
    "        # Where mask is False/0, replace with large negative number\n",
    "        scaled_scores = np.where(mask, scaled_scores, -1e9)\n",
    "    \n",
    "    # Step 4: Apply softmax to get attention weights (probabilities)\n",
    "    # Each row now sums to 1.0 across seq_len_k dimension\n",
    "    # This converts similarity scores to a probability distribution\n",
    "    attention_weights = softmax(scaled_scores, axis=-1)\n",
    "    \n",
    "    # Step 5: Compute weighted sum of values\n",
    "    # Each output position is a weighted average of all value vectors\n",
    "    # Weights determined by attention_weights\n",
    "    # Result shape: (batch_size, seq_len_q, d_v)\n",
    "    output = np.matmul(attention_weights, V)\n",
    "    \n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a72678",
   "metadata": {},
   "source": [
    "# DEMO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a501c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 2\n",
      "Sequence length: 4\n",
      "Key/Query dimension (d_k): 8\n",
      "Value dimension (d_v): 8\n",
      "Q shape: (2, 4, 8)\n",
      "K shape: (2, 4, 8)\n",
      "V shape: (2, 4, 8)\n",
      " Attention without mask\n",
      "\n",
      "Output shape: (2, 4, 8)\n",
      "Attention weights shape: (2, 4, 4)\n",
      "\n",
      "Attention weights (first batch):\n",
      "[[0.25239951 0.24751685 0.25106345 0.24902019]\n",
      " [0.24893051 0.25202596 0.24813962 0.25090392]\n",
      " [0.24710991 0.25421605 0.24841189 0.25026215]\n",
      " [0.25241656 0.24979312 0.24903318 0.24875714]]\n",
      "Row sums: [1. 1. 1. 1.]\n",
      "\n",
      "Output (first batch, first sequence position):\n",
      "[-0.02728092  0.00473303 -0.04275996 -0.07967607  0.03838312  0.06356303\n",
      " -0.08637104  0.06873783]\n",
      "\n",
      "------------------------------------------------------------\n",
      "Example 2: Attention with causal mask\n",
      "(Each position can only attend to itself and previous positions)\n",
      "\n",
      "Causal mask (first batch):\n",
      "[[1 0 0 0]\n",
      " [1 1 0 0]\n",
      " [1 1 1 0]\n",
      " [1 1 1 1]]\n",
      "\n",
      "Attention weights with causal mask (first batch):\n",
      "[[1.         0.         0.         0.        ]\n",
      " [0.49691046 0.50308954 0.         0.        ]\n",
      " [0.32959509 0.33907325 0.33133167 0.        ]\n",
      " [0.25241656 0.24979312 0.24903318 0.24875714]]\n",
      "\n",
      "Note: Upper triangle is ~0 (masked positions)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def main():\n",
    "    \n",
    "    \n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Define dimensions\n",
    "    batch_size = 2\n",
    "    seq_len = 4\n",
    "    d_k = 8  # dimension of queries and keys\n",
    "    d_v = 8  # dimension of values\n",
    "    \n",
    "    print(f\"Batch size: {batch_size}\")\n",
    "    print(f\"Sequence length: {seq_len}\")\n",
    "    print(f\"Key/Query dimension (d_k): {d_k}\")\n",
    "    print(f\"Value dimension (d_v): {d_v}\")\n",
    "    \n",
    "    # Create sample Q, K, V matrices with small random values\n",
    "    Q = np.random.randn(batch_size, seq_len, d_k) * 0.1\n",
    "    K = np.random.randn(batch_size, seq_len, d_k) * 0.1\n",
    "    V = np.random.randn(batch_size, seq_len, d_v) * 0.1\n",
    "    \n",
    "    print(f\"Q shape: {Q.shape}\")\n",
    "    print(f\"K shape: {K.shape}\")\n",
    "    print(f\"V shape: {V.shape}\")\n",
    "    \n",
    "    # Example 1: Without mask\n",
    "    \n",
    "    print(\" Attention without mask\")\n",
    "    \n",
    "    \n",
    "    output, attention_weights = scaled_dot_product_attention(Q, K, V)\n",
    "    \n",
    "    print(f\"\\nOutput shape: {output.shape}\")\n",
    "    print(f\"Attention weights shape: {attention_weights.shape}\")\n",
    "    \n",
    "    print(\"\\nAttention weights (first batch):\")\n",
    "    print(attention_weights[0])\n",
    "    print(f\"Row sums: {attention_weights[0].sum(axis=1)}\")\n",
    "    \n",
    "    print(\"\\nOutput (first batch, first sequence position):\")\n",
    "    print(output[0, 0])\n",
    "    \n",
    "    # Example 2: With mask (e.g., masking future positions)\n",
    "\n",
    "    print(\"\\n\" + \"-\" * 60)\n",
    "    print(\"Example 2: Attention with causal mask\")\n",
    "    print(\"(Each position can only attend to itself and previous positions)\")\n",
    "    \n",
    "    # Create causal mask: lower triangular matrix\n",
    "    # True for positions that should be attended to\n",
    "    causal_mask = np.tril(np.ones((seq_len, seq_len)), k=0).astype(bool)\n",
    "    # Expand to batch dimension\n",
    "    causal_mask = np.broadcast_to(causal_mask, (batch_size, seq_len, seq_len))\n",
    "    \n",
    "    print(\"\\nCausal mask (first batch):\")\n",
    "    print(causal_mask[0].astype(int))\n",
    "    \n",
    "    output_masked, attention_weights_masked = scaled_dot_product_attention(\n",
    "        Q, K, V, mask=causal_mask\n",
    "    )\n",
    "    \n",
    "    print(\"\\nAttention weights with causal mask (first batch):\")\n",
    "    print(attention_weights_masked[0])\n",
    "    print(\"\\nNote: Upper triangle is 0 (masked positions)\")\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
